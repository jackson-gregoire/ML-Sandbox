{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rf.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ByOvR4iutj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "import sklearn as sk\n",
        "from sklearn import model_selection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux0rroTSkl6c"
      },
      "source": [
        "1. In this problem, we consider Random Forests. Implement the Random Forestalgorithm with 100 decision stumps. You must implement the decision stumps from scratch.Use information gain as the splitting measure. Apply your Random Forest implementationto the cancer dataset described above and do the following:\n",
        "  *   Use m = 3 random attributes to determine the split of your decision stumps.  Learn amodel for an increasing number of decision stumps in the ensemble.  Compute the trainand test set classification error as the number of decision stumps increases.\n",
        "  *   Vary the number of random attributes m from \\\\\n",
        "  {2, . . . , p = 10} and fit a model using 100 decision stumps.  Compute the train and test set classification error as the numberof random featuresmincreases.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txRi_hoC7TIk"
      },
      "source": [
        "def random_forest(X, y, learners, m):\n",
        "  np.random.seed(5525)\n",
        "  X_original = X\n",
        "  y_original = y\n",
        "  prediction_thresholds = []\n",
        "  split_feature_indices = []\n",
        "  signs = []\n",
        "  iter_error = []\n",
        "    \n",
        "  for t in range(learners):\n",
        "    index = np.random.choice(X.shape[0], X.shape[0], replace=True)\n",
        "    D_t = X[index]\n",
        "    y = y_original[index]\n",
        "    feature_subset = get_feature_subset(X, m)\n",
        "    entropy = []\n",
        "    conditional_entropy = []\n",
        "\n",
        "    for feature in range(feature_subset.shape[1]):\n",
        "      entropy = np.append(entropy, get_entropy(feature_subset[:, feature])) \n",
        "      #input(\"Press Enter to continue...\")\n",
        "      conditional_entropy = np.append(conditional_entropy, get_conditional_entropy(feature_subset[:, feature], y))\n",
        "\n",
        "    information_gain = get_information_gain(entropy, conditional_entropy)\n",
        "    max_index = np.argmax(information_gain)\n",
        "    split_feature_indices = np.append(split_feature_indices, max_index)\n",
        "    split_feature = feature_subset[:,max_index]\n",
        "    \n",
        "    # Method 1: With Binary Classification\n",
        "    #confusion = confusion_matrix(split_feature, y)\n",
        "    #threshold = binary(split_feature, confusion, y)\n",
        "    #sign = get_sign(split_feature, threshold, y)\n",
        "\n",
        "    # Method 2\n",
        "    threshold, sign = best_threshold(split_feature, y)\n",
        "    \n",
        "    prediction_thresholds = np.append(prediction_thresholds, threshold)\n",
        "    signs = np.append(signs, sign)\n",
        "    learner_consensus = _predict_one(threshold, split_feature, sign)\n",
        "    error = np.sum(np.where(learner_consensus != y, 1, 0))/X.shape[0]\n",
        "    # IF YOU WANT TO MUTE TRAINING CLASSIFICATION ERROR YOU CAN COMMENT THE \n",
        "    # PRINT CALL BELOW\n",
        "    print(\"Classification error for stump\", t+1, \": \", error)\n",
        "    iter_error = np.append(iter_error, error)\n",
        "\n",
        "  return prediction_thresholds, split_feature_indices, signs, iter_error\n",
        "\n",
        "\n",
        "def get_feature_subset(X, m):\n",
        "  cols = np.random.randint(0, X.shape[1], m)\n",
        "\n",
        "  return X[:,cols]\n",
        "\n",
        "\n",
        "def _predict_one(threshold, feature, sign):\n",
        "  if sign == 1:\n",
        "    predictions = np.where(feature >= threshold, 1, -1)\n",
        "      #_ , counts = np.unique(predictions, return_counts=True)\n",
        "      #votes = np.append(votes, counts[np.argmax(counts)])\n",
        "    \n",
        "  else:\n",
        "    predictions = np.where(feature < threshold, 1, -1)\n",
        "\n",
        "      #_ , counts = np.unique(predictions, return_counts=True)\n",
        "      #votes = np.append(votes, counts[np.argmax(counts)])\n",
        "\n",
        "  elements, counts = np.unique(predictions, return_counts= True)\n",
        "  index = np.argmax(counts)\n",
        "\n",
        "  return elements[index]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        " Each weak learner is going to generate (# of rows in the the data) votes. Basically, 1 votes per sample \n",
        " in the data. Then we're going to have T weak learners, so we will have a (#Rows x T)\n",
        " matrix correspond to the votes of each learner for the given sample. Then\n",
        " we just take the consensus. \n",
        "\"\"\"\n",
        "def predict(X, thresholds, feature_indices,signs):\n",
        "  learner_votes = []\n",
        "  learner_consensus = []\n",
        "  \n",
        "  for threshold, index, sign in zip(thresholds, feature_indices, signs):\n",
        "    if sign == 1:\n",
        "      predictions = np.where(X[:,int(index)] >= threshold, 1, -1)\n",
        "      learner_votes = np.append(learner_votes, predictions)\n",
        "      #_ , counts = np.unique(predictions, return_counts=True)\n",
        "      #votes = np.append(votes, counts[np.argmax(counts)])\n",
        "    \n",
        "    else:\n",
        "      predictions = np.where(X[:,index] < threshold, 1, -1)\n",
        "      learner_votes = np.append(learner_votes, predictions)\n",
        "\n",
        "      #_ , counts = np.unique(predictions, return_counts=True)\n",
        "      #votes = np.append(votes, counts[np.argmax(counts)])\n",
        "\n",
        "  learner_votes = np.reshape(learner_votes, (X.shape[0], signs.shape[0]))\n",
        "  \n",
        "  for row in learner_votes:\n",
        "    elements, counts = np.unique(row, return_counts= True)\n",
        "    index = np.argmax(counts)\n",
        "    learner_consensus = np.append(learner_consensus, elements[index])\n",
        "\n",
        "  return learner_consensus\n",
        "\n",
        "\n",
        "def get_sign(feature, threshold, y):\n",
        "  predictions = np.array([np.where(feature <  threshold, 1, -1),\n",
        "                          np.where(feature >= threshold, 1, -1)])\n",
        "  errors = []\n",
        "\n",
        "  for guesses in predictions:\n",
        "    error = 0\n",
        "    \n",
        "    for guess, label in zip(guesses, y):\n",
        "      if guess != label:\n",
        "        error += 1\n",
        "\n",
        "    errors = np.append(errors, error/len(y))\n",
        "\n",
        "  return np.argmin(errors) + 1\n",
        "\n",
        "def get_entropy(feature):\n",
        "  unique_elements, counts = np.unique(feature, return_counts=True)\n",
        "  p_x = counts/np.sum(counts)\n",
        "\n",
        "  return -(p_x.dot(np.log2(p_x)))\n",
        "\n",
        "\n",
        "# Returns unsummed element entropys\n",
        "def _get_entropys(p_x1, p_x2):\n",
        "  \n",
        "  return -(p_x1*np.log2(p_x1) + p_x2*np.log2(p_x2))\n",
        "\n",
        "def get_conditional_entropy(feature, y):\n",
        "  unique_y, counts_y = np.unique(y, return_counts=True)\n",
        "  unique_x, counts_x = np.unique(feature, return_counts=True)\n",
        "  confusion = confusion_matrix(feature, y)\n",
        "  p_x = counts_x/np.sum(counts_x)\n",
        "  joint_entropy = np.array([])\n",
        "\n",
        "  for rows in confusion:\n",
        "    p_x1 = rows[0]/len(feature)\n",
        "    p_x2 = rows[1]/len(feature)\n",
        "    \n",
        "    if p_x1 == 0 or p_x2 == 0:\n",
        "      joint_entropy = np.append(joint_entropy, 0)\n",
        "\n",
        "    else:\n",
        "      joint_entropy = np.append(joint_entropy, _get_entropys(p_x1, p_x1))\n",
        "\n",
        "  conditional_entropy = p_x.dot(joint_entropy)\n",
        "\n",
        "  return conditional_entropy\n",
        "\n",
        "def get_information_gain(entropy, conditional_entropy):\n",
        "  return entropy - conditional_entropy\n",
        "\n",
        "\n",
        "def confusion_matrix(feature, y):\n",
        "  unique_x, counts_x = np.unique(feature, return_counts=True)\n",
        "  max = np.argmax(unique_x)\n",
        "  confusion_positive = np.zeros(unique_x[max])\n",
        "  confusion_negative = np.zeros(unique_x[max])\n",
        "  \n",
        "  for i,j in zip(feature, y):\n",
        "    if j == 1:\n",
        "      confusion_positive[i - 1] += 1\n",
        "\n",
        "    elif j == -1:\n",
        "      confusion_negative[i - 1] += 1\n",
        "  \n",
        "  confusion = np.vstack((confusion_positive, confusion_negative)).T\n",
        "\n",
        "  return confusion[~np.all(confusion == 0, axis = 1)]\n",
        "\n",
        "\n",
        "def binary(feature, confusion, y):\n",
        "  unique_x, counts_x = np.unique(feature, return_counts=True)\n",
        "  unique_y, counts_y = np.unique(y, return_counts=True)\n",
        "  # {-1, 1}\n",
        "  priors = counts_y/sum(counts_y)\n",
        "  posterior = []\n",
        "  \n",
        "  for rows in confusion:\n",
        "    p = priors[1]*(rows[0]/len(feature)) + priors[0]*(rows[1]/len(feature))\n",
        "    posterior = np.append(posterior, p)\n",
        "\n",
        "  threshold_index = unique_x[np.argmax(posterior)]\n",
        "  \n",
        "  return threshold_index\n",
        "\n",
        "def best_threshold(feature, y):\n",
        "  unique_x, counts_x = np.unique(feature, return_counts=True)\n",
        "  unique_y, counts_y = np.unique(y, return_counts=True)\n",
        "  # Sorts in ascending order\n",
        "  sorted_elements = np.sort(unique_x)\n",
        "  index = 1\n",
        "  thresholds = []\n",
        "  errors_above = []\n",
        "  errors_below = []\n",
        "  \n",
        "\n",
        "  for i in range(len(sorted_elements) - 2):\n",
        "    thresholds = np.append(thresholds, (sorted_elements[index - 1] + sorted_elements[index]) / 2 )\n",
        "    index += 1\n",
        "\n",
        "  for i in thresholds:\n",
        "    # Value >= threshold --> -1\n",
        "    predictions_above = np.where(feature >= i, 1, -1)\n",
        "    error = np.where(predictions_above != y, 1, 0)\n",
        "    errors_above = np.append(errors_above, np.sum(error)/len(y))\n",
        "    # Value < threshold --> +1\n",
        "    predictions_below = np.where(feature < i, 1, -1)\n",
        "    error = np.where(predictions_below != y, 1, 0)\n",
        "    errors_below = np.append(errors_below, np.sum(error)/len(y))\n",
        "\n",
        "  min_above_index = np.argmin(errors_above)\n",
        "  min_above_error = errors_above[min_above_index]\n",
        "  min_below_index = np.argmin(errors_below)\n",
        "  min_below_error = errors_below[min_below_index]\n",
        "\n",
        "  # Below --> Sign 2\n",
        "  if min_below_error < min_above_error:\n",
        "    return thresholds[min_below_index], 2 \n",
        "\n",
        "  # Above --> Sign 1\n",
        "  else:\n",
        "    return thresholds[min_above_index], 1\n",
        "\n",
        "\n",
        "def main():\n",
        "  np.set_printoptions(threshold=np.inf)\n",
        "  plt.figure(figsize=(10,8))\n",
        "  bc_data = np.loadtxt(\"/content/drive/MyDrive/breast-cancer-wisconsin-mode.csv\", dtype=int,delimiter = \",\")\n",
        "  X = bc_data[:,1:-1]\n",
        "  y = bc_data[:,-1]\n",
        "  y = np.where(y == 2, 1, -1)\n",
        "  X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size = 0.2, train_size = 0.8)\n",
        "  training_errors = []\n",
        "  test_errors = []\n",
        "\n",
        "  # You can change the number of iterations and features here\n",
        "  thresholds, features, signs, iter_error = random_forest(X_train, y_train, 100, 3)\n",
        "  thresholds = thresholds.astype(\"int32\")\n",
        "  features = features.astype(\"int32\")\n",
        "  signs = signs.astype(\"int32\")\n",
        "  learner_consensus = predict(X_train, thresholds, features, signs)\n",
        "  training_error = np.sum(np.where(learner_consensus != y_train, 1, 0))/X_train.shape[0]\n",
        "  test_consensus = predict(X_test, thresholds, features, signs)\n",
        "  error = np.sum(np.where(test_consensus != y_test, 1, 0))/X_test.shape[0]\n",
        "  print(\"Test error: \", error)\n",
        "  p = [2,3,4,5,6,7,8,9]\n",
        "\n",
        "  for i in p:\n",
        "    print()\n",
        "    print(\"Varying feature subset, m = \", i, \":\")\n",
        "    thresholds, features, signs, iter_error = random_forest(X_train, y_train, 100, i)\n",
        "    thresholds = thresholds.astype(\"int32\")\n",
        "    features = features.astype(\"int32\")\n",
        "    signs = signs.astype(\"int32\")\n",
        "    learner_consensus = predict(X_train, thresholds, features, signs)\n",
        "    training_error = np.sum(np.where(learner_consensus != y_train, 1, 0))/X_train.shape[0]\n",
        "    training_errors = np.append(training_errors, training_error)\n",
        "    test_consensus = predict(X_test, thresholds, features, signs)\n",
        "    error = np.sum(np.where(test_consensus != y_test, 1, 0))/X_test.shape[0]\n",
        "    test_errors = np.append(test_errors, error)\n",
        "    print(\"Test error: \", error)\n",
        "    x = np.linspace(0, 100, num = 100)\n",
        "    plt.plot(x, iter_error)\n",
        "    plt.xlabel(\"Decision Trees\")\n",
        "    plt.ylabel(\"Classification Error\")\n",
        "\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(10,8))\n",
        "  plt.plot(p, training_errors, color = \"green\")\n",
        "  plt.plot(p, test_errors, color = \"red\")\n",
        "  plt.xlabel(\"Random Features\")\n",
        "  plt.ylabel(\"Classification Errror\")\n",
        "  plt.legend(labels = (\"Training Error\", \"Testing Error\"))\n",
        "  plt.show\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}