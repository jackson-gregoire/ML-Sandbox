{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5525_HW3_P3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "secIKrYhD_if"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtUeSI2LEFI2"
      },
      "source": [
        "# Problem 3\n",
        "Implement a convolutional neural network with the following specifications.\n",
        "• Input: 1-channel input, size 28x28  \n",
        "• Convolution layer: Convolution kernel size is (3, 3) with stride as 1. Input channels - 1; Output channels - 20 nodes  \n",
        "• ReLU activation function  \n",
        "• Max-pool: 2x2 max pool  \n",
        "• Dropout layer with probability p = 0.50  \n",
        "• Flatten input for feed to fully connected layers  \n",
        "• Fully connected layer 1: flattened input with bias; output - 128 nodes  \n",
        "• ReLU activation function  \n",
        "• Dropout layer with probability p = 0.50  \n",
        "• Fully connected layer 2: input - 128 nodes; output - 10 nodes  \n",
        "• Softmax activation function  \n",
        "• Use cross entropy as loss function  \n",
        "\n",
        "For this problem, we will be experimenting with a variety of parameters.\n",
        "First, train using SGD as the optimizer and mini batches of size 32. Plot the cumulative training loss and accuracy for every epoch. Once training is complete, apply the learned model to the test set and report the testing accuracy.\n",
        "Second, train your network using mini batch sizes of [32, 64, 96, 128] and plot the convergence run time vs mini batch sizes for each of the following optimizers: SGD, Adagrad, and Adam. You should report 3 figures, one for each optimizer where each figure has mini batch size on the x-axis and the convergence run time on the y-axis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80_s5RCYEKod"
      },
      "source": [
        "def cnn():\n",
        "  mnist = tf.keras.datasets.mnist\n",
        "  (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "  # Rescale\n",
        "  X_train = X_train / 255.0\n",
        "  X_test = X_test / 255.0\n",
        "  X_train = X_train.reshape((-1,28,28,1))\n",
        "  X_test = X_test.reshape((-1,28,28,1))\n",
        "\n",
        "  # Optimizer\n",
        "  # Best eta 0.01: ~0.95\n",
        "  # eta 0.001: ~0.91\n",
        "  SGD = tf.optimizers.SGD(learning_rate = 0.01)\n",
        "  \n",
        "  # Best eta 0.01: ~0.96\n",
        "  # eta 0.001: ~0.11\n",
        "  Adagrad = tf.optimizers.Adagrad(learning_rate= 0.01)\n",
        "  \n",
        "  # Best eta 0.009 : ~0.95\n",
        "  # eta 0.001: ~ 0.11\n",
        "  Adam = tf.optimizers.Adam(learning_rate = 0.009)\n",
        "  opts = [SGD, Adagrad, Adam]\n",
        "  opt_names = [\"SGD\", \"Adagrad\", \"Adam\"]\n",
        "  batch = [32, 64, 96, 128]\n",
        "\n",
        "  \n",
        "  # Testing Optimizers\n",
        "  for i, names in zip(opts, opt_names):\n",
        "    model = build_model()\n",
        "    time_elapsed = []\n",
        "\n",
        "    # Testing Batches\n",
        "    for j in batch:\n",
        "      print(\"_\"*100)\n",
        "      print(\"OPTIMIZER: \", names)\n",
        "      print(\"BATCH SIZE: \", j)\n",
        "      print(\"_\"*100)\n",
        "      print()\n",
        "      model.compile(optimizer=i,\n",
        "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "      \n",
        "      # TO DO: Tweek converge criterion\n",
        "      # Callbacks for convergence criterion\n",
        "      es = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta = 0.001, verbose = 1)\n",
        "      checkpoint = tf.keras.callbacks.ModelCheckpoint(\"best_model\", monitor = 'accuracy' ,verbose = 1, mode = 'max', save_best_only = True)\n",
        "      callbacks = [es, checkpoint]\n",
        "      start_time = time.clock()\n",
        "      history = model.fit(X_train, y_train, batch_size = j, epochs = 100, callbacks = [es, checkpoint])\n",
        "      end_time = time.clock()\n",
        "      time_elapsed.append(end_time - start_time)\n",
        "      test_loss, test_acc = model.evaluate(X_test,  y_test, verbose=1) \n",
        "      print(\"=\"*100)\n",
        "      print(\"Test Accuracy: \", test_acc)\n",
        "      print(\"=\"*100)\n",
        "\n",
        "      if (i == SGD and j == 32):\n",
        "        plot_sgd_ctl(history)\n",
        "\n",
        "      # Destroy the graph\n",
        "      keras.backend.clear_session()\n",
        "\n",
        "    plot_batch_time(time_elapsed, batch, names)  \n",
        "\n",
        "\n",
        "def build_model():\n",
        "  model = keras.Sequential([\n",
        "                            keras.layers.Conv2D(filters= 20, kernel_size=3, data_format='channels_last', activation='relu',input_shape = (28,28,1)),\n",
        "                            keras.layers.MaxPool2D(pool_size = 2),\n",
        "                            keras.layers.Dropout(rate = 0.5),\n",
        "                            keras.layers.Flatten(),\n",
        "                            keras.layers.Dense(units = 128, activation = 'relu', use_bias=True),\n",
        "                            keras.layers.Dropout(rate = 0.5),\n",
        "                            keras.layers.Dense(units = 10, activation='softmax')\n",
        "                            ])\n",
        "                            \n",
        "  return model\n",
        "\n",
        "# Graphs the Culumative Training Loss for SGD\n",
        "def plot_sgd_ctl(history):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.title('SGD: Model Accuracy/Loss per Epoch')\n",
        "  plt.ylabel('Accuracy/Loss')\n",
        "  plt.xlabel('Epoch #')\n",
        "  plt.legend(['accuracy', 'loss'], loc='upper left')\n",
        "  plt.grid()\n",
        "  plt.show() \n",
        "  # Uncomment when you want to save\n",
        "  # plt.savefig(\"sgd_ctl_plot.png\")\n",
        "\n",
        "# Graphs Batch vs Time\n",
        "def plot_batch_time(time, batch, name):\n",
        "  plt.plot(batch, time)\n",
        "  plt.title(f\"{name}: Time vs Batch Size\")\n",
        "  plt.ylabel(\"Time to Convergence\")\n",
        "  plt.xlabel(\"Batch Size\")\n",
        "  plt.grid()\n",
        "  plt.show()\n",
        "  # Uncomment when you want to save\n",
        "  # plt.savefig(f\"{name}_plot.png\")\n",
        "\n",
        "\n",
        "def main():\n",
        "  cnn()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}